{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'And': 0, 'cold.': 1, 'sunken': 2, 'deep': 3, 'warm': 4, 'Were': 5, 'dig': 6, 'and': 7, 'asked,': 8, 'an': 9, 'thy': 10, \"youth's\": 11, 'own': 12, 'mine': 13, 'where': 14, 'shame,': 15, 'in': 16, 'When': 17, 'beauty': 18, 'days;': 19, 'eyes,': 20, \"feel'st\": 21, 'made': 22, 'winters': 23, 'small': 24, 'How': 25, 'Shall': 26, 'livery': 27, 'lusty': 28, \"totter'd\": 29, 'a': 30, 'lies,': 31, 'trenches': 32, 'sum': 33, 'If': 34, 'fair': 35, 'gazed': 36, 'weed': 37, 'thou': 38, 'now,': 39, 'on': 40, 'use,': 41, 'thine': 42, 'held:': 43, 'when': 44, 'old': 45, 'by': 46, 'see': 47, 'Where': 48, 'to': 49, \"'This\": 50, 'it': 51, 'To': 52, 'thriftless': 53, 'be': 54, 'within': 55, 'besiege': 56, 'treasure': 57, 'being': 58, 'art': 59, 'make': 60, 'This': 61, 'praise': 62, 'answer': 63, 'of': 64, 'child': 65, 'new': 66, 'much': 67, \"deserv'd\": 68, 'field,': 69, 'brow,': 70, 'shall': 71, 'more': 72, 'say,': 73, \"beauty's\": 74, 'worth': 75, 'all-eating': 76, 'my': 77, 'all': 78, 'couldst': 79, 'Proving': 80, 'thine!': 81, 'old,': 82, 'forty': 83, 'succession': 84, 'the': 85, \"excuse,'\": 86, 'blood': 87, 'so': 88, 'Will': 89, 'praise.': 90, 'were': 91, 'his': 92, 'count,': 93, 'Then': 94, 'Thy': 95, 'proud': 96}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        print(embeds.shape)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 20])\n",
      "[520.7556076049805, 518.3876376152039]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('attenwhore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "728330e26e9121d5b50e3854b1188ab90c7ba41dbab680e56e74aded888e54d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
